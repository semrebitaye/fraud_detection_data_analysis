{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_data = pd.read_csv('/home/semre/fraud_detection_data_analysis/data/Fraud_Data.csv')\n",
    "ip_data = pd.read_csv('/home/semre/fraud_detection_data_analysis/data/IpAddress_to_Country.csv')\n",
    "creditcard_data = pd.read_csv('/home/semre/fraud_detection_data_analysis/data/creditcard.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a. Feature and Target Separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and targets for both datasets\n",
    "# Credit card dataset\n",
    "X_creditcard = creditcard_data.drop(columns=['Class'])  # Features\n",
    "y_creditcard = creditcard_data['Class']  # Target\n",
    "\n",
    "# Fraud data dataset\n",
    "X_fraud = fraud_data.drop(columns=['class'])  # Features\n",
    "y_fraud = fraud_data['class']  # Target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train-test split for credit card data\n",
    "X_train_cc, X_test_cc, y_train_cc, y_test_cc = train_test_split(X_creditcard, y_creditcard, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train-test split for fraud data\n",
    "X_train_fraud, X_test_fraud, y_train_fraud, y_test_fraud = train_test_split(X_fraud, y_fraud, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### a. Logistic Regression, Decision Tree, Random Forest, and Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import hstack\n",
    "\n",
    "def preprocess_data(X_train, X_test, date_columns=None, cat_columns=None):\n",
    "    # Convert date columns if they exist\n",
    "    if date_columns:\n",
    "        for col in date_columns:\n",
    "            X_train[col] = pd.to_datetime(X_train[col])\n",
    "            X_test[col] = pd.to_datetime(X_test[col])\n",
    "            X_train[f'{col}_hour'] = X_train[col].dt.hour\n",
    "            X_train[f'{col}_day'] = X_train[col].dt.day\n",
    "            X_test[f'{col}_hour'] = X_test[col].dt.hour\n",
    "            X_test[f'{col}_day'] = X_test[col].dt.day\n",
    "            X_train.drop(columns=[col], inplace=True)\n",
    "            X_test.drop(columns=[col], inplace=True)\n",
    "\n",
    "    # Handle categorical features using OneHotEncoder\n",
    "    if cat_columns:\n",
    "        encoder = OneHotEncoder(sparse=True, handle_unknown='ignore')  # Keep sparse\n",
    "        X_train_cat = encoder.fit_transform(X_train[cat_columns])\n",
    "        X_test_cat = encoder.transform(X_test[cat_columns])\n",
    "        \n",
    "        # Drop the original categorical columns\n",
    "        X_train.drop(columns=cat_columns, inplace=True)\n",
    "        X_test.drop(columns=cat_columns, inplace=True)\n",
    "        \n",
    "        # Concatenate the sparse matrices with the original DataFrames\n",
    "        X_train = hstack([X_train, X_train_cat])\n",
    "        X_test = hstack([X_test, X_test_cat])\n",
    "    \n",
    "    # Scale the numerical features\n",
    "    scaler = StandardScaler(with_mean=False)  # Use with_mean=False to avoid issues with sparse data\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    return X_train_scaled, X_test_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### b. Multi-Layer Perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def preprocess_data(X_train, X_test, date_columns=None, cat_columns=None):\n",
    "    # Convert date columns if they exist\n",
    "    if date_columns:\n",
    "        for col in date_columns:\n",
    "            X_train[col] = pd.to_datetime(X_train[col])\n",
    "            X_test[col] = pd.to_datetime(X_test[col])\n",
    "\n",
    "            # Extract useful datetime features (e.g., hour, day)\n",
    "            X_train[f'{col}_hour'] = X_train[col].dt.hour\n",
    "            X_train[f'{col}_day'] = X_train[col].dt.day\n",
    "            X_test[f'{col}_hour'] = X_test[col].dt.hour\n",
    "            X_test[f'{col}_day'] = X_test[col].dt.day\n",
    "\n",
    "            # Drop original date columns if not needed\n",
    "            X_train.drop(columns=[col], inplace=True)\n",
    "            X_test.drop(columns=[col], inplace=True)\n",
    "    \n",
    "    # Handle categorical features using OneHotEncoder\n",
    "    if cat_columns:\n",
    "        # Apply one-hot encoding to categorical columns\n",
    "        encoder = OneHotEncoder(sparse_output=True, handle_unknown='ignore')  # Change made here\n",
    "        X_train_cat = encoder.fit_transform(X_train[cat_columns])\n",
    "        X_test_cat = encoder.transform(X_test[cat_columns])\n",
    "        \n",
    "        # Convert to DataFrame and reset index to merge\n",
    "        X_train_cat_df = pd.DataFrame(X_train_cat.toarray(), columns=encoder.get_feature_names_out(cat_columns), index=X_train.index)\n",
    "        X_test_cat_df = pd.DataFrame(X_test_cat.toarray(), columns=encoder.get_feature_names_out(cat_columns), index=X_test.index)\n",
    "        \n",
    "        # Drop the original categorical columns and merge the encoded data\n",
    "        X_train = X_train.drop(columns=cat_columns).join(X_train_cat_df)\n",
    "        X_test = X_test.drop(columns=cat_columns).join(X_test_cat_df)\n",
    "    \n",
    "    # Scale the data using StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. MLOps Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024/10/20 13:41:18 WARNING mlflow.models.signature: Failed to infer the model signature from the input example. Reason: NotFittedError(\"This LogisticRegression instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\"). To see the full traceback, set the logging level to DEBUG via `logging.getLogger(\"mlflow\").setLevel(logging.DEBUG)`. To disable automatic signature inference, set `signature` to `False` in your `log_model` or `save_model` call.\n",
      "2024/10/20 13:41:21 WARNING mlflow.models.model: Failed to validate serving input example {\n",
      "  \"dataframe_split\": {\n",
      "    \"columns\": [\n",
      "      \"feature1\",\n",
      "      \"feature2\"\n",
      "    ],\n",
      "    \"data\": [\n",
      "      [\n",
      "        0.5,\n",
      "        1.5\n",
      "      ]\n",
      "    ]\n",
      "  }\n",
      "}. Alternatively, you can avoid passing input example and pass model signature instead when logging the model. To ensure the input example is valid prior to serving, please try calling `mlflow.models.validate_serving_input` on the model uri and serving input example. A serving input example can be generated from model input example using `mlflow.models.convert_input_example_to_serving_input` function.\n",
      "Got error: This LogisticRegression instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Set experiment name\n",
    "mlflow.set_experiment(\"fraud-detection\")\n",
    "\n",
    "# Start MLflow run\n",
    "with mlflow.start_run():\n",
    "    # Example: Assuming log_reg is your trained logistic regression model\n",
    "    log_reg = LogisticRegression(max_iter=1000)  # Your model initialization and training goes here\n",
    "\n",
    "    # Create an example input DataFrame (adjust the columns as per your model)\n",
    "    example_input = pd.DataFrame({\n",
    "        'feature1': [0.5],  # Replace 'feature1', 'feature2', etc., with actual feature names\n",
    "        'feature2': [1.5],\n",
    "        # Add more features as needed\n",
    "    })\n",
    "\n",
    "    # Log the model with input example\n",
    "    mlflow.sklearn.log_model(log_reg, \"logistic_regression\", input_example=example_input)\n",
    "\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"max_iter\", 1000)\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"accuracy\", accuracy_score(y_test_cc, y_pred_cc))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
